{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7811be84",
   "metadata": {},
   "source": [
    "# ***$$技术报告$$***\n",
    "<center>碱基配对</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7e0ad7",
   "metadata": {},
   "source": [
    "#### 一、注意事项\n",
    "\n",
    "比赛指定python版本==3.8\\\n",
    "必须使用torch框架,gpu/cpu版本皆可\\\n",
    "cuda版本不限（若有）\\\n",
    "不限使用其他的库\n",
    "\n",
    "选手只可上交一个\"技术报告.ipynb\"文件，该文件包含选手从序列到活性预测的全过程代码+说明（如下框架）\\\n",
    "若有其余数据，则需要放在同一个目录，一同打包为zip文件上传,代码中数据路径为相对路径\n",
    "\n",
    "算力资源需要选手自行解决\\\n",
    "注意该程序的可复现性，审核人员会对程序进行复现操作\\\n",
    "本程序上交主办方后，主办方在三个月（自上交之日算起）内会对其内容保密。三个月后会公开优秀算法，以供后续大家学习\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f788e",
   "metadata": {},
   "source": [
    "#### 二、包的安装\n",
    "选手需要把配置（environment）写明，下面是例子\\\n",
    "pytorch==2.4.1\\\n",
    "pytorch-cuda==12.1\\\n",
    "pandas==2.0.3\\\n",
    "numpy==1.24.3\\\n",
    "scikit-learn==1.3.2\\\n",
    "biopython==1.78\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b52252",
   "metadata": {},
   "source": [
    "#### 三、依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fc9da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca13d7",
   "metadata": {},
   "source": [
    "**确定随机数种子**  \n",
    "保证每次运行结果相同\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f389c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#确定随机种子，保证结果的可重复性\n",
    "seed=66\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71403027",
   "metadata": {},
   "source": [
    "#### 四、代码主体部分（可有多个主体，按逻辑划分）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaed59b",
   "metadata": {},
   "source": [
    "##### 完整流程概述\n",
    "\n",
    "根据GFP蛋白质序列预测其荧光亮度，采用深度学习方法构建回归模型。整体流程包括数据读取、模型构建、训练与保存，具体步骤如下：\n",
    "\n",
    "1. **数据读取与解析**\n",
    "   - 使用 `Bio.SeqIO` 模块从FASTA格式的文件中读取氨基酸序列及其log亮度值；\n",
    "   - 编写 `read_gfp_data_with_bio` 函数进行解析，提取序列列表与对应亮度。\n",
    "\n",
    "2. **数据预处理**\n",
    "   - 构建`apply_mutations_to_wt`处理要预测的数据，将其装换为序列；\n",
    "   - 对氨基酸序列进行编码，将其转换为数字ID；\n",
    "   - 构建 `encode_amino_acids` 函数，将氨基酸序列映射\n",
    "   为数字ID列表。\n",
    "   - 对数据集进行划分,训练集占比为0.8，测试集占比为0.2，训练集再划分出验证集，验证集占比为0.125，并对亮度值进行归一化处理。\n",
    "\n",
    "   \n",
    "3. **模型构建**\n",
    "   - 定义神经网络类 `GFPBrightnessPredictor`，继承自 `nn.Module`；\n",
    "   - 包含以下模块：\n",
    "     - 嵌入层 `nn.Embedding`：将氨基酸ID映射为稠密向量；\n",
    "     - 双向LSTM `nn.LSTM`：提取局部与全局序列特征；\n",
    "     - 注意力机制 `nn.Sequential`：聚焦关键位置；\n",
    "     - 全连接层 `nn.Linear`：输出亮度值；\n",
    "     - 使用 `LayerNorm` 进行归一化增强稳定性。\n",
    "\n",
    "4. **模型训练**\n",
    "   - 构建训练数据加载器 `TensorDataset` 与 `DataLoader`；\n",
    "   - 选用损失函数 `HuberLoss`；\n",
    "   - 使用 `Adam` 优化器和 `ReduceLROnPlateau` 学习率调度器；\n",
    "   - 迭代训练模型若干轮，每轮更新权重、评估性能。\n",
    "5. **模型保存**\n",
    "   - 训练完成后，将模型参数保存到 `gfp_model1.pt` 文件中。\n",
    "6. **模型预测**\n",
    "   - 加载保存的模型参数；\n",
    "   - 对新数据进行预测，输出亮度值。\n",
    "   - 排序输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "802945ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取到141144条序列\n"
     ]
    }
   ],
   "source": [
    "#读取和解析亮度序列\n",
    "def read_gfp_data_with_bio(fasta_path):\n",
    "    sequences = []\n",
    "    brightness = []\n",
    "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        header = record.id  # 如：avGFP-3.719212132-41.23189601673761-WT\n",
    "        parts = header.split('-')\n",
    "        log_bright = float(parts[1])  # 第二段是log亮度\n",
    "        brightness.append(log_bright)\n",
    "        sequences.append(str(record.seq))  # 氨基酸序列\n",
    "    return sequences, brightness\n",
    "train_sequences,train_brightness = read_gfp_data_with_bio('GFP_sequences.fasta')\n",
    "print(\"读取到{}条序列\".format(len(train_sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "669af7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理要预测的数据\n",
    "def apply_mutations_to_wt(wt_seq: str, mutation_str: str) -> str:\n",
    "    \"\"\"将突变字符串应用到 WT 序列\"\"\"\n",
    "    seq = list(wt_seq)\n",
    "    mutations = mutation_str.strip().split(':')\n",
    "    \n",
    "    for mut in mutations:\n",
    "        # 提取数字部分和氨基酸部分\n",
    "        pos_str = ''\n",
    "        aa = ''\n",
    "        for char in mut:\n",
    "            if char.isdigit():\n",
    "                pos_str += char\n",
    "            else:\n",
    "                aa = char\n",
    "                break  # 一旦遇到非数字字符就停止\n",
    "        \n",
    "        if pos_str and aa:\n",
    "            pos = int(pos_str) - 1  # 1-based 转 0-based\n",
    "            if 0 <= pos < len(seq):\n",
    "                seq[pos] = aa\n",
    "    \n",
    "    return ''.join(seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1048f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#要预测的突变\n",
    "mutants = [\n",
    "    '5N:73H:163G:34A:90H:206V',\n",
    "    '11Y:157G:158Q',\n",
    "    '5N:73H:163G:34V:117G:206M',\n",
    "    '',\n",
    "    '5N:73H:163G:34A:90E:206M',\n",
    "    '154P:157S:214M',\n",
    "    '93L:157T:158E',\n",
    "    '154I:157A:214M',\n",
    "    '157T:158Q:214M',\n",
    "    '5N:34V:72T:73H:97I:117N:142Q:158Q:163G:171M:175G:206I:214R',\n",
    "    '5N:34V:73H:117N:128T:142Q:153N:163G:171M:206I',\n",
    "    '5N:73H:163G:34V:142Q:206I:90D:117N:203Y',\n",
    "    '5N:73H:163G:34V:142G:206V',\n",
    "    '5N:73H:163G:34V:142Q:206I:90D:117K:128V',\n",
    "    '5N:34V:43S:72T:73H:117N:142Q:163G:171M:175G:206I:214R:237A',\n",
    "    '5N:34V:43S:72T:73H:97G:117N:142Q:163G:171M:206I:214R',\n",
    "    '5N:73H:163G:34V:142Q:206I:90E:117N:214H:63S:170N:171M:39S:43S:64M:175S',\n",
    "    '5N:73H:163G:34V:142Q:206I:90D:117N:203L',\n",
    "    '5N:34V:43S:72T:73H:117N:142Q:163G:171M:175A:206I:214R',\n",
    "    '93L:157T:158N',\n",
    "]\n",
    "#野生型\n",
    "wt_seq='MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK'\n",
    "#生成突变体序列\n",
    "pred_sequences=[apply_mutations_to_wt(wt_seq,mutant) for mutant in mutants]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15d97354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#转换模块\n",
    "AA_TO_ID = {aa: i for i, aa in enumerate(\"ACDEFGHIKLMNPQRSTVWY\")}\n",
    "# 定义填充标识的ID，用于在编码氨基酸序列时填充到指定长度，此处填充ID为20\n",
    "PAD_ID = 20\n",
    "# 定义函数，将氨基酸序列编码为整数序列\n",
    "def encode_seqs(seqs, max_len=256):\n",
    "    result = np.full((len(seqs), max_len), PAD_ID, dtype=np.int32)  # 提前填充好\n",
    "    for i, seq in enumerate(seqs):\n",
    "        for j, aa in enumerate(seq[:max_len]):\n",
    "            result[i, j] = AA_TO_ID.get(aa, PAD_ID)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6327fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#进行装换\n",
    "max_len=256\n",
    "train_seqs=encode_seqs(train_sequences,max_len)\n",
    "pred_seqs=encode_seqs(pred_sequences,max_len)\n",
    "#将标签转换为numpy数组\n",
    "train_brightness=np.array(train_brightness)\n",
    "#划分训练集和测试集\n",
    "train_seqs,test_seqs,train_labels,test_labels=train_test_split(train_seqs,train_brightness,test_size=0.2,random_state=0)\n",
    "#标签Z-score标准化\n",
    "scaler=StandardScaler()\n",
    "train_labels=scaler.fit_transform(train_labels.reshape(-1,1))\n",
    "test_labels=scaler.transform(test_labels.reshape(-1,1))\n",
    "#划分验证集和训练集\n",
    "train_seqs,val_seqs,train_labels,val_labels=train_test_split(train_seqs,train_labels,test_size=0.125,random_state=0)\n",
    "#将数据转换为PyTorch张量\n",
    "\n",
    "train_seq = torch.tensor(train_seqs, dtype=torch.long)\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.float32).squeeze(-1)\n",
    "\n",
    "val_seq = torch.tensor(val_seqs, dtype=torch.long)\n",
    "val_labels = torch.tensor(val_labels, dtype=torch.float32).squeeze(-1)\n",
    "\n",
    "test_seq = torch.tensor(test_seqs, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.float32).squeeze(-1)\n",
    "\n",
    "train_dataset = TensorDataset(train_seq, train_labels)\n",
    "val_dataset = TensorDataset(val_seq, val_labels)\n",
    "test_dataset = TensorDataset(test_seq, test_labels)\n",
    "\n",
    "batch_size=128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "383afd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义神经网络\n",
    "class GFPBrightnessPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size=21, embed_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        # 1. 嵌入层：将序列中的数字转为稠密向量\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_ID)\n",
    "        # 归一化\n",
    "        self.emb_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # 2. 双向 LSTM：捕捉局部 + 全局序列特征\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # 3. 注意力池化聚焦关键位点\n",
    "        self.attention=nn.Sequential(\n",
    "            nn.Linear(hidden_dim*2,64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64,1),\n",
    "        )\n",
    "\n",
    "        # 4. 全连接层：回归亮度\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 128),  # 因为是双向 LSTM，维度是 2*hidden\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),#每次丢弃概率为0.3，防止过拟合\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        self.dropout=nn.Dropout(0.3)\n",
    "    def forward(self, x):\n",
    "        # 输入 x 的维度为 (batch_size, seq_len)，表示一批输入序列，每个序列长度为 seq_len\n",
    "        x = self.embedding(x)  # 经过嵌入层处理后，输出维度变为 (batch_size, seq_len, embed_dim)\n",
    "        x = self.emb_norm(x)  # 对嵌入层的输出进行归一化处理\n",
    "        out, _ = self.lstm(x)  # 经过双向 LSTM 层处理后，输出维度变为 (batch_size, seq_len, hidden_dim*2)，因为是双向 LSTM，所以维度翻倍\n",
    "        out = self.dropout(out) #每次丢弃概率为0.3，防止过拟合\n",
    "        # 进行注意力池化操作，聚焦序列中的关键位点\n",
    "        attention=self.attention(out).softmax(dim=1)  # 经过注意力网络处理并在维度 1 上进行 softmax 操作，输出维度为 (batch_size, seq_len, 1)\n",
    "        pooled=(out*attention).sum(dim=1)  # 通过注意力权重对 LSTM 输出进行加权求和，得到池化后的结果\n",
    "        return self.fc(pooled).squeeze(-1)  # 经过全连接层处理并去掉最后一个维度，输出维度为 (batch,)，得到最终的预测结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9211e4c",
   "metadata": {},
   "source": [
    "#### 模型原理  \n",
    "1.序列编码：将 21 种氨基酸映射为 64 维嵌入向量，捕捉残基间语义关系（如极性、疏水性）。  \n",
    "2.双向 LSTM 建模：通过门控机制捕获序列前后文依赖，提取局部和全局特征（如关键功能域）。  \n",
    "3.注意力聚焦：自适应权重分配机制，自动识别对荧光起决定作用的残基（如 Ser65、Tyr66）。  \n",
    "4.非线性回归：多层感知器将序列特征映射到连续亮度值，捕捉残基间复杂物理化学相互作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32d3c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 早停策略\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=1e-4, path='gfp_model1.pt'):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            torch.save(model.state_dict(), self.path)  # 保存当前最优模型\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be5a2026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | 当前学习率: 0.001000 | Train Loss: 0.3053 | Val Loss: 0.2617\n",
      "Epoch 2 | 当前学习率: 0.001000 | Train Loss: 0.2408 | Val Loss: 0.1800\n",
      "Epoch 3 | 当前学习率: 0.001000 | Train Loss: 0.1711 | Val Loss: 0.1291\n",
      "Epoch 4 | 当前学习率: 0.001000 | Train Loss: 0.1210 | Val Loss: 0.0904\n",
      "Epoch 5 | 当前学习率: 0.001000 | Train Loss: 0.0925 | Val Loss: 0.0707\n",
      "Epoch 6 | 当前学习率: 0.001000 | Train Loss: 0.0765 | Val Loss: 0.0638\n",
      "Epoch 7 | 当前学习率: 0.001000 | Train Loss: 0.0691 | Val Loss: 0.0587\n",
      "Epoch 8 | 当前学习率: 0.001000 | Train Loss: 0.0614 | Val Loss: 0.0635\n",
      "Epoch 9 | 当前学习率: 0.001000 | Train Loss: 0.0557 | Val Loss: 0.0530\n",
      "Epoch 10 | 当前学习率: 0.001000 | Train Loss: 0.0545 | Val Loss: 0.0441\n",
      "Epoch 11 | 当前学习率: 0.001000 | Train Loss: 0.0480 | Val Loss: 0.0449\n",
      "Epoch 12 | 当前学习率: 0.001000 | Train Loss: 0.0479 | Val Loss: 0.0417\n",
      "Epoch 13 | 当前学习率: 0.001000 | Train Loss: 0.0520 | Val Loss: 0.0453\n",
      "Epoch 14 | 当前学习率: 0.001000 | Train Loss: 0.0449 | Val Loss: 0.0417\n",
      "Epoch 15 | 当前学习率: 0.001000 | Train Loss: 0.0437 | Val Loss: 0.0441\n",
      "Epoch 16 | 当前学习率: 0.001000 | Train Loss: 0.0436 | Val Loss: 0.0387\n",
      "Epoch 17 | 当前学习率: 0.001000 | Train Loss: 0.0414 | Val Loss: 0.0379\n",
      "Epoch 18 | 当前学习率: 0.001000 | Train Loss: 0.0401 | Val Loss: 0.0389\n",
      "Epoch 19 | 当前学习率: 0.001000 | Train Loss: 0.0389 | Val Loss: 0.0390\n",
      "Epoch 20 | 当前学习率: 0.001000 | Train Loss: 0.0386 | Val Loss: 0.0378\n",
      "Epoch 21 | 当前学习率: 0.001000 | Train Loss: 0.0366 | Val Loss: 0.0352\n",
      "Epoch 22 | 当前学习率: 0.001000 | Train Loss: 0.0359 | Val Loss: 0.0331\n",
      "Epoch 23 | 当前学习率: 0.001000 | Train Loss: 0.0361 | Val Loss: 0.0335\n",
      "Epoch 24 | 当前学习率: 0.001000 | Train Loss: 0.0372 | Val Loss: 0.0373\n",
      "Epoch 25 | 当前学习率: 0.001000 | Train Loss: 0.0342 | Val Loss: 0.0332\n",
      "Epoch 26 | 当前学习率: 0.000500 | Train Loss: 0.0332 | Val Loss: 0.0359\n",
      "Epoch 27 | 当前学习率: 0.000500 | Train Loss: 0.0313 | Val Loss: 0.0319\n",
      "Epoch 28 | 当前学习率: 0.000500 | Train Loss: 0.0287 | Val Loss: 0.0307\n",
      "Epoch 29 | 当前学习率: 0.000500 | Train Loss: 0.0296 | Val Loss: 0.0318\n",
      "Epoch 30 | 当前学习率: 0.000500 | Train Loss: 0.0289 | Val Loss: 0.0291\n",
      "Epoch 31 | 当前学习率: 0.000500 | Train Loss: 0.0285 | Val Loss: 0.0290\n",
      "Epoch 32 | 当前学习率: 0.000500 | Train Loss: 0.0277 | Val Loss: 0.0294\n",
      "Epoch 33 | 当前学习率: 0.000500 | Train Loss: 0.0274 | Val Loss: 0.0298\n",
      "Epoch 34 | 当前学习率: 0.000500 | Train Loss: 0.0285 | Val Loss: 0.0310\n",
      "Epoch 35 | 当前学习率: 0.000500 | Train Loss: 0.0275 | Val Loss: 0.0274\n",
      "Epoch 36 | 当前学习率: 0.000500 | Train Loss: 0.0263 | Val Loss: 0.0285\n",
      "Epoch 37 | 当前学习率: 0.000500 | Train Loss: 0.0267 | Val Loss: 0.0295\n",
      "Epoch 38 | 当前学习率: 0.000500 | Train Loss: 0.0269 | Val Loss: 0.0275\n",
      "Epoch 39 | 当前学习率: 0.000250 | Train Loss: 0.0276 | Val Loss: 0.0353\n",
      "Epoch 40 | 当前学习率: 0.000250 | Train Loss: 0.0274 | Val Loss: 0.0283\n",
      "验证集性能停止提升，触发早停！\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GFPBrightnessPredictor().to(device)\n",
    "#使用Huber损失函数，结合了均方误差（MSE）和绝对误差（MAE）的优点。\n",
    "loss_fn = nn.HuberLoss(delta=1.0)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "# 学习率调度器\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "#早停策略\n",
    "early_stopping = EarlyStopping(patience=5, delta=1e-3, path='gfp_model1.pt')\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    # 训练\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * x.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # 验证集上的损失\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "            val_loss += loss.item() * x.size(0)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    # 学习率调整\n",
    "    scheduler.step(val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1} | 当前学习率: {current_lr:.6f} \",end='|')\n",
    "    print(f\" Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    # 早停策略\n",
    "    early_stopping(val_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"验证集性能停止提升，触发早停！\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cad62d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\32840\\AppData\\Local\\Temp\\ipykernel_40756\\3669852407.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('gfp_model1.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集损失: 0.0295\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "model.load_state_dict(torch.load('gfp_model1.pt'))\n",
    "\n",
    "# 测试集评估\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        test_loss += loss.item() * x.size(0)\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print(f\"测试集损失: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8bfdd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个序列的预测亮度(log)\n",
      "序列 1: 3.6704061031341553\n",
      "序列 2: 3.6637730598449707\n",
      "序列 3: 3.693673610687256\n",
      "序列 4: 3.722771644592285\n",
      "序列 5: 3.692875385284424\n",
      "序列 6: 3.661938428878784\n",
      "序列 7: 3.559307336807251\n",
      "序列 8: 3.6814284324645996\n",
      "序列 9: 3.704284429550171\n",
      "序列 10: 3.6232423782348633\n",
      "序列 11: 3.6284070014953613\n",
      "序列 12: 3.6079185009002686\n",
      "序列 13: 3.6657724380493164\n",
      "序列 14: 3.653627872467041\n",
      "序列 15: 3.6309969425201416\n",
      "序列 16: 3.6186511516571045\n",
      "序列 17: 3.494438648223877\n",
      "序列 18: 3.6082353591918945\n",
      "序列 19: 3.6251914501190186\n",
      "序列 20: 3.6141223907470703\n",
      "按照预测值大小排序的序号列表:\n",
      "[17, 7, 12, 18, 20, 16, 10, 19, 11, 15, 14, 6, 2, 13, 1, 8, 5, 3, 9, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\32840\\AppData\\Local\\Temp\\ipykernel_40756\\832542042.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"gfp_model1.pt\"))\n"
     ]
    }
   ],
   "source": [
    "#加载进行预测\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_len=256\n",
    "model = GFPBrightnessPredictor().to(device)\n",
    "model.load_state_dict(torch.load(\"gfp_model1.pt\"))\n",
    "model.eval()\n",
    "# 进行预测\n",
    "encode_preds=encode_seqs(pred_sequences,max_len)\n",
    "preds=torch.tensor(encode_preds,dtype=torch.long).to(device)\n",
    "with torch.no_grad():\n",
    "    predictions = model(preds)\n",
    "    predictions=scaler.inverse_transform(predictions.cpu().numpy().reshape(-1, 1))\n",
    "    predictions = predictions.squeeze().tolist()\n",
    "    print(\"每个序列的预测亮度(log)\")\n",
    "    for i, pred in enumerate(predictions):\n",
    "        print(f\"序列 {i+1}: {pred}\")\n",
    "    sorted_indices = np.argsort(predictions)\n",
    "    sorted_indices_list = (sorted_indices+1).tolist()\n",
    "    print(\"按照预测值大小排序的序号列表:\")\n",
    "    print(sorted_indices_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1048ffc2",
   "metadata": {},
   "source": [
    "#### 五、模型运行结果（即排序结果，需要和提交的表格中数据对应！）\n",
    "4\\\n",
    "9\\\n",
    "3\\\n",
    "5\\\n",
    "8\\\n",
    "1\\\n",
    "13\\\n",
    "2\\\n",
    "6\\\n",
    "14\\\n",
    "15\\\n",
    "11\\\n",
    "19\\\n",
    "10\\\n",
    "16\\\n",
    "20\\\n",
    "18\\\n",
    "12\\\n",
    "7\\\n",
    "17\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a69f11d",
   "metadata": {},
   "source": [
    "#### 六、参考资料"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2debfa29",
   "metadata": {},
   "source": [
    "[1]https://jupyter.org/ <br>\n",
    "[2]pytorch教程 https://www.runoob.com/pytorch/pytorch-tutorial.html<br>\n",
    "[3]Hu, H.; Li, Z.; Elofsson, A.; Xie, S. A Bi-LSTM Based Ensemble Algorithm for Prediction of Protein Secondary Structure. Appl. Sci. 2019, 9, 3538. https://doi.org/10.3390/app9173538<br>\n",
    "[4]Wang.; Generating New Protein Sequences by Using Dense Network and Attention Mechanism\n",
    "Wang et al.<br>\n",
    "[5]深度学习模型：LSTM (Long Short-Term Memory) - 长短时记忆网络详解 https://blog.csdn.net/2301_80840905/article/details/144107247<br>\n",
    "[6]神经网络算法 —— Embedding（嵌入）！！ https://blog.csdn.net/leonardotu/article/details/136165819"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3_8_forDeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
